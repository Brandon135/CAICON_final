{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Mapping, Union, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "subsets = [\n",
    "    path for path in glob.glob(\"testdata/*\") if os.path.isdir(path)\n",
    "]\n",
    "\n",
    "def coco2dataset():\n",
    "    with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "\n",
    "    for idx, data in enumerate(datas):\n",
    "        # image = Image.open(f\"testdata/{img['file_name']}\")\n",
    "        image = Image.open(data[\"image\"])\n",
    "\n",
    "        yield {\n",
    "            \"image_id\": idx,\n",
    "            \"width\": data['width'],\n",
    "            \"height\": data['height'],\n",
    "            \"objects\": data[\"objects\"],\n",
    "            \"image\": image,\n",
    "        }\n",
    "\n",
    "categories = [\"crack\", \"pothole\"]\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "dataset = Dataset.from_generator(coco2dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection, get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "model_name_or_path = \"PekingU/rtdetr_r50vd\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    do_resize=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    anchor_image_size=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "\n",
    "max_size = 640\n",
    "train_augmentation_and_transform = A.Compose(\n",
    "    [\n",
    "        A.ShiftScaleRotate(p=0.3),\n",
    "        A.Compose(\n",
    "            [\n",
    "                A.SmallestMaxSize(max_size=max_size, p=1.0),\n",
    "                A.RandomSizedBBoxSafeCrop(height=max_size, width=max_size, p=1.0),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Blur(blur_limit=7, p=0.5),\n",
    "                A.MotionBlur(blur_limit=7, p=0.5),\n",
    "                A.Defocus(radius=(1, 5), alias_blur=(0.1, 0.25), p=0.1),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "        A.Perspective(p=0.2),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.HueSaturationValue(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.RGBShift(p=0.1),\n",
    "        A.ChannelShuffle(p=0.1),\n",
    "        A.Emboss(p=0.1),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n",
    ")\n",
    "\n",
    "\n",
    "# to make sure boxes are clipped to image size and there is no boxes with area < 1 pixel\n",
    "validation_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0]:\n",
    "    image = dataset[i][\"image\"]\n",
    "    annotations = dataset[i][\"objects\"]\n",
    "\n",
    "    # Apply the augmentation\n",
    "    output = train_augmentation_and_transform(image=np.array(image), bboxes=annotations[\"bbox\"], category=annotations[\"category\"])\n",
    "\n",
    "    # Unpack the output\n",
    "    image = Image.fromarray(output[\"image\"])\n",
    "    categories, boxes = output[\"category\"], output[\"bboxes\"]\n",
    "\n",
    "    # Draw the augmented image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for category, box in zip(categories, boxes):\n",
    "        x, y, w, h = box\n",
    "        draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
    "        draw.text((x, y), id2label[category], fill=\"fuchsia\")\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CPPE5Dataset(Dataset):\n",
    "    def __init__(self, dataset, image_processor, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.image_processor = image_processor\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def format_image_annotations_as_coco(image_id, categories, boxes):\n",
    "        annotations = []\n",
    "        for category, bbox in zip(categories, boxes):\n",
    "            formatted_annotation = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category,\n",
    "                \"bbox\": list(bbox),\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": bbox[2] * bbox[3],\n",
    "            }\n",
    "            annotations.append(formatted_annotation)\n",
    "\n",
    "        return {\n",
    "            \"image_id\": image_id,\n",
    "            \"annotations\": annotations,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        image_id = sample[\"image_id\"]\n",
    "        image = sample[\"image\"]\n",
    "        boxes = sample[\"objects\"][\"bbox\"]\n",
    "        categories = sample[\"objects\"][\"category\"]\n",
    "\n",
    "        # Convert image to RGB numpy array\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=boxes, category=categories)\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            categories = transformed[\"category\"]\n",
    "\n",
    "        # Format annotations in COCO format for image_processor\n",
    "        formatted_annotations = self.format_image_annotations_as_coco(image_id, categories, boxes)\n",
    "\n",
    "        # Apply the image processor transformations: resizing, rescaling, normalization\n",
    "        result = self.image_processor(\n",
    "            images=image, annotations=formatted_annotations, return_tensors=\"pt\"\n",
    "        )\n",
    "        result = {k: v[0] for k, v in result.items()}\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "def collate_fn(batch: list) -> Mapping[str, Union[torch.Tensor, List[Any]]]:\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "    return data\n",
    "    \n",
    "\n",
    "train_dataset = CPPE5Dataset(dataset, image_processor, transform=train_augmentation_and_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n], \"lr\": 1e-4},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"backbone\" in n], \"lr\": 1e-5},\n",
    "])\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=500, num_training_steps=10000\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_step = 0\n",
    "bpar = tqdm(total=10000)\n",
    "for epoch in range(300):\n",
    "    for data in train_loader:\n",
    "        data = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in data.items()}\n",
    "        data[\"labels\"] = [{k: v.to(device) for k, v in labels.items()} for labels in data[\"labels\"]]\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(**data)\n",
    "            loss = outputs[\"loss\"]\n",
    "        \n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "        # scheduler.step()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        cur_step += 1\n",
    "        if cur_step % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "        bpar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ckpt_rtdetr_r50vd_1epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Mapping, Union, Any\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection, get_cosine_schedule_with_warmup\n",
    "\n",
    "def num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name_or_path = \"PekingU/rtdetr_r50vd\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    do_resize=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"ckpt_rtdetr_r50vd_75epoch\",\n",
    ").eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = glob.glob(\"data/test/images/*.jpg\")\n",
    "print(f\"Found {len(image_files)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "submission = []\n",
    "for idx in tqdm(range(0, len(image_files), batch_size)):\n",
    "    # image = Image.open(image_file)\n",
    "\n",
    "    batch_images = image_files[idx:idx+batch_size]\n",
    "    images = [Image.open(image_file) for image_file in batch_images]\n",
    "\n",
    "    # Preprocess the image and the annotations\n",
    "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post process the results\n",
    "    results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1] for image in images]), threshold=0.2)\n",
    "\n",
    "    # Draw boxes on the image\n",
    "    # draw = ImageDraw.Draw(image)\n",
    "    # for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "    #     box = [round(i, 2) for i in box.tolist()]\n",
    "    #     draw.rectangle(box, outline=\"red\", width=3)\n",
    "    #     draw.text((box[0], box[1]), f\"{model.config.id2label[label.item()]}: {round(score.item(), 3)}\", fill=\"red\")\n",
    "    # image.show()\n",
    "    \n",
    "    for image_file, result in zip(batch_images, results):\n",
    "        labels = []\n",
    "\n",
    "        keep = nms(result[\"boxes\"], result[\"scores\"], iou_threshold=0.5)\n",
    "        for ridx, (score, label, box) in enumerate(zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"])):\n",
    "            if ridx not in keep:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = box\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            xc = x1 + w / 2\n",
    "            yc = y1 + h / 2\n",
    "\n",
    "            xc = float(xc / image.size[0])\n",
    "            yc = float(yc / image.size[1])\n",
    "            w = float(w / image.size[0])\n",
    "            h = float(h / image.size[1])\n",
    "\n",
    "            labels.append({\"class_id\": label.item(), \"conf\": score.item(), \"x\": xc, \"y\": yc, \"w\": w, \"h\": h})\n",
    "            print(labels)\n",
    "        image_id = os.path.basename(image_file).split(\".\")[0]\n",
    "        submission.append({\"id\": image_id, \"labels\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"205submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    len(s[\"labels\"]) for s in submission\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    k:v.shape for k, v in inputs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "\n",
    "for i, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    image_id = row[\"id\"]\n",
    "    labels = []\n",
    "    for label in row[\"labels\"]:\n",
    "        class_id = label[\"class_id\"]\n",
    "        x = label[\"x\"]\n",
    "        y = label[\"y\"]\n",
    "        w = label[\"w\"]\n",
    "        h = label[\"h\"]\n",
    "        conf = label[\"conf\"]\n",
    "\n",
    "        labels.append({\"class_id\": class_id, \"conf\": conf, \"x\": x, \"y\": y, \"w\": w, \"h\": h})\n",
    "    submission.append({\"id\": image_id, \"labels\": labels})\n",
    "\n",
    "df_submission = pd.DataFrame(submission)\n",
    "df_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
